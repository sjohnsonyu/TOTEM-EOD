{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.decomposition import PCA\n",
    "import sklearn\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = '/n/holylabs/LABS/krajan_lab/Users/sjohnsonyu/elephantfish-talking-storage/totem_vars/'\n",
    "\n",
    "dataset = 'real_fish_day_12ms_eods_only_seq_len_640'\n",
    "mask_ratio = 0.25\n",
    "seed = 11\n",
    "compression_factor = 4\n",
    "\n",
    "exp_id = f'{dataset}_mr{mask_ratio}_seed{seed}_f{compression_factor}'\n",
    "\n",
    "codebook = joblib.load(storage_dir + f'{exp_id}_codebook.pkl')\n",
    "codes = joblib.load(storage_dir + f'{exp_id}_codes.pkl')\n",
    "code_ids = joblib.load(storage_dir + f'{exp_id}_code_ids.pkl')\n",
    "preds = joblib.load(storage_dir + f'{exp_id}_preds.pkl')\n",
    "trues = joblib.load(storage_dir + f'{exp_id}_trues.pkl')\n",
    "inps = joblib.load(storage_dir + f'{exp_id}_inps.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codebook = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_codebook.pkl')\n",
    "# codes = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_codes.pkl')\n",
    "# code_ids = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_code_ids.pkl')\n",
    "# preds = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_preds.pkl')\n",
    "# trues = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_trues.pkl')\n",
    "\n",
    "# codebook = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_seed{seed}_codebook.pkl')\n",
    "# codes = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_seed{seed}_codes.pkl')\n",
    "# code_ids = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_seed{seed}_code_ids.pkl')\n",
    "# preds = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_seed{seed}_preds.pkl')\n",
    "# trues = joblib.load(storage_dir + f'{dataset}_mr{mask_ratio}_seed{seed}_trues.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_preds = torch.cat(preds, dim=0)\n",
    "concatenated_trues = torch.cat(trues, dim=0)\n",
    "\n",
    "thresholded_preds_all = np.where(concatenated_preds.cpu() > 0.5, 1, 0)\n",
    "trues_all = concatenated_trues.cpu().numpy()\n",
    "\n",
    "print('percentage of trues that are 1:', np.sum(trues_all) / len(trues_all))\n",
    "print('percentage of preds that are 1:', np.sum(thresholded_preds_all) / len(thresholded_preds_all))\n",
    "\n",
    "# precision and recall and f1 by class\n",
    "metrics = sklearn.metrics.precision_recall_fscore_support(trues_all, thresholded_preds_all, average=None)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': metrics[0],\n",
    "    'Recall': metrics[1],\n",
    "    'F1-Score': metrics[2],\n",
    "    'Support': metrics[3]\n",
    "})\n",
    "print(metrics_df.round(3))\n",
    "f1 = metrics_df['F1-Score'].mean()\n",
    "print('Overall F1-Score:', f1.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(codes)\n",
    "# codes_pca = pca.transform(codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "def get_sklearn_report(preds, trues):\n",
    "    metrics = sklearn.metrics.precision_recall_fscore_support(trues, preds, average=None)\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': metrics[0],\n",
    "        'Recall': metrics[1],\n",
    "        'F1-Score': metrics[2],\n",
    "        'Support': metrics[3]\n",
    "    })\n",
    "    print(metrics_df.round(3))\n",
    "    f1 = metrics_df['F1-Score'].mean()\n",
    "    print('Overall F1-Score:', f1.round(3))\n",
    "\n",
    "\n",
    "def run_baselines(test_data):\n",
    "    test_data = test_data.flatten()\n",
    "    all_0_preds = np.zeros_like(test_data)\n",
    "    all_1_preds = np.ones_like(test_data)\n",
    "    p_1 = np.sum(test_data) / test_data.size\n",
    "    random_preds = np.random.choice([0, 1], size=test_data.size, p=[1-p_1, p_1])\n",
    "    print('All 0s')\n",
    "    get_sklearn_report(all_0_preds, test_data)\n",
    "    print('All 1s')\n",
    "    get_sklearn_report(all_1_preds, test_data)\n",
    "    print('Random')\n",
    "    get_sklearn_report(random_preds, test_data)\n",
    "\n",
    "run_baselines(trues_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def find_incorrect_predictions_with_context(inputs, preds, labels, window_size=5):\n",
    "#     inputs_flat = inputs.view(-1)\n",
    "#     preds_flat = preds.view(-1)\n",
    "#     labels_flat = labels.view(-1)\n",
    "#     mask_indices = (inputs_flat == -1).nonzero(as_tuple=True)[0]\n",
    "#     masked_preds = preds_flat[mask_indices]\n",
    "#     masked_labels = labels_flat[mask_indices]\n",
    "#     incorrect_indices = (masked_preds != masked_labels).nonzero(as_tuple=True)[0]\n",
    "#     incorrect_contexts = []\n",
    "#     for idx in incorrect_indices:\n",
    "#         print('idx:', idx)\n",
    "#         if idx > 1000: continue\n",
    "#         start = max(0, mask_indices[idx] - window_size)\n",
    "#         end = min(inputs_flat.size(0), mask_indices[idx] + window_size + 1)\n",
    "#         print('start:', start)\n",
    "#         print('end:', end)\n",
    "#         context_input = inputs_flat[start:end]\n",
    "#         context_pred = preds_flat[start:end]\n",
    "#         context_label = labels_flat[start:end]\n",
    "#         incorrect_contexts.append((context_input, context_pred, context_label))\n",
    "#     return incorrect_indices, incorrect_contexts\n",
    "\n",
    "# # Example usage\n",
    "# incorrect_indices, incorrect_contexts = find_incorrect_predictions_with_context(inps[0], preds[0], trues[0])\n",
    "# for idx, (context_input, context_pred, context_label) in zip(incorrect_indices, incorrect_contexts):\n",
    "#     print(f\"Incorrect prediction at index {idx}\")\n",
    "#     print(f\"Context input: {context_input}\")\n",
    "#     print(f\"Context prediction: {context_pred}\")\n",
    "#     print(f\"Context true label: {context_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_incorrect_predictions_with_context(inputs, preds, labels, window_size=5):\n",
    "    inputs_np = inputs.detach().cpu().numpy().flatten()  # Convert tensor to NumPy array and flatten\n",
    "    preds_np = preds.detach().cpu().numpy().flatten()\n",
    "    labels_np = labels.detach().cpu().numpy().flatten()\n",
    "    mask_indices = np.where(inputs_np == -1)[0]  # Find indices where the input was masked with -1\n",
    "    masked_preds = preds_np[mask_indices]\n",
    "    masked_labels = labels_np[mask_indices]\n",
    "    incorrect_indices = np.where(masked_preds != masked_labels)[0]\n",
    "    incorrect_contexts = []\n",
    "    try:\n",
    "        for idx in incorrect_indices:\n",
    "            start = max(0, mask_indices[idx] - window_size)\n",
    "            end = min(inputs_np.size, mask_indices[idx] + window_size + 1)\n",
    "            context_input = inputs_np[start:end]\n",
    "            context_pred = preds_np[start:end]\n",
    "            context_label = labels_np[start:end]\n",
    "            incorrect_contexts.append((context_input, context_pred, context_label))\n",
    "    except Exception as e:\n",
    "        # print('start', start)\n",
    "        # print('end', end)\n",
    "        # print('inputs_np.shape', inputs_np.shape)\n",
    "        # print('preds_np.shape', preds_np.shape)\n",
    "        print('hello')\n",
    "        print(idx)\n",
    "        print(mask_indices)\n",
    "        print(e)\n",
    "    return incorrect_indices, incorrect_contexts\n",
    "\n",
    "# Example usage (assuming inputs, preds, and labels are PyTorch tensors)\n",
    "incorrect_indices, incorrect_contexts = find_incorrect_predictions_with_context(inps[0], preds[0], trues[0])\n",
    "\n",
    "for idx, (context_input, context_pred, context_label) in zip(incorrect_indices, incorrect_contexts):\n",
    "    print(f\"Incorrect prediction at index {idx}\")\n",
    "    print(f\"Context input: {context_input}\")\n",
    "    print(f\"Context prediction: {context_pred}\")\n",
    "    print(f\"Context true label: {context_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "totem-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
